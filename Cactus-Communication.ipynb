{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "552e7b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9011a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sumiran\n",
      "[nltk_data]     Grover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8041920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "import re\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file while removing unwanted metadata.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        page_text = page.get_text(\"text\")\n",
    "        \n",
    "        # Remove figure numbers, table numbers, and references\n",
    "        page_text = re.sub(r'Fig\\s*\\d+|Table\\s*\\d+', '', page_text)\n",
    "        page_text = re.sub(r'References|Bibliography', '', page_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        text += page_text + \"\\n\"\n",
    "\n",
    "    # Remove multiple spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96fa4fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Preprocesses extracted text by removing journal metadata, references, and unnecessary special characters.\"\"\"\n",
    "    \n",
    "    # Remove journal metadata (e.g., ISSN, impact factor, volume, issue)\n",
    "    text = re.sub(r'eISSN:\\s*\\d+|pISSN:\\s*\\d+|Impact Factor.*?\\d+\\.\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'Volume:\\s*\\d+\\s*Issue:\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'ISO\\s*\\d{4,}', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "    # Remove in-text citations like [1], (Smith et al., 2020)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\d{4}.*?\\)', '', text)\n",
    "\n",
    "    # Remove extra special characters but keep basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?;:()\\s]', '', text)\n",
    "\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56982131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def split_into_sections(text):\n",
    "    \"\"\"Splits research paper text into logical sections using common headers.\"\"\"\n",
    "    \n",
    "    # More robust section names to detect variations\n",
    "    section_headers = {\n",
    "        \"Title\": [\"title\"],\n",
    "        \"Abstract\": [\"abstract\"],\n",
    "        \"Introduction\": [\"introduction\"],\n",
    "        \"Methods\": [\"methods\", \"methodology\", \"materials & methods\"],\n",
    "        \"Results\": [\"results\", \"findings\"],\n",
    "        \"Discussion\": [\"discussion\", \"analysis\"],\n",
    "        \"Conclusion\": [\"conclusion\", \"summary\", \"final thoughts\"]\n",
    "    }\n",
    "\n",
    "    # Initialize empty sections\n",
    "    sections = {key: \"\" for key in section_headers}\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "\n",
    "    current_section = \"Title\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        for section, keywords in section_headers.items():\n",
    "            if any(keyword in sentence.lower() for keyword in keywords):\n",
    "                current_section = section\n",
    "                break  # Switch to new section\n",
    "\n",
    "        # Append sentence to detected section\n",
    "        sections[current_section] += sentence + \" \"\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d420c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"A_Comprehensive_Review_of_Unimodal_and_Multimodal_Fingerprint_Biometric_Authentication_Systems_Fusion_Attacks_and_Template_Protection.pdf\"  # Change to your file path\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "cleaned_text = clean_text(raw_text)\n",
    "sections = split_into_sections(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d14c0904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Title ===\n",
      "Received 7 March 2024, accepted 25 April 2024, date of publication 30 April 2024, date of current version 13 May 2024. Digital Object Identifier 10.1109ACCESS.2024.3395417 A Comprehensive Review of Unimodal and Multimodal Fingerprint Biometric Authentication Systems: Fusion, Attacks, and Template Protection U. SUMALATHA 1, K. KRISHNA PRAKASHA 1, , Manipal Academy of Higher Education , Manipal Academy of Higher Education  and Srikanth Prabhu  Universality: Anyone must be able to use the applica t...\n",
      "\n",
      "\n",
      "=== Abstract ===\n",
      "...\n",
      "\n",
      "\n",
      "=== Introduction ===\n",
      "A. K. Jain, A. Ross, and S. Prabhakar, An introduction to biometric recognition, IEEE Trans. Circuits Syst. Video Technol., vol. 14, no. 1, pp. 420, Jan. 2004. A. K. Jain, A. Ross, and K. Nandakumar, Introduction to Biometrics. Cham, Switzerland: Springer, 2016. K. ElMaleh and W. ElHajj, Voice biometrics: Security, forensics, and healthcare, J. Med. Syst., vol. 43, no. 9, p. 306, 2019. K. Banerjee, J. P. Singh, and R. Kaur, Human retinal identification: Review and future scope, J. Comput. Sci. T...\n",
      "\n",
      "\n",
      "=== Methods ===\n",
      "The and describes fingerprint feature extraction traditional and deep learning methods. Fingerprint classification has evolved since the inception the of computer technology. Henrys classification method stands out as the most widely employed approach in finger print classification. Over time, this method has progressed to the automated fingerprint identification system  Fingerprint Indexing using Texture: The ridge flow structure, ridge frequency field, ridge pattern types, ridge orientation fi...\n",
      "\n",
      "\n",
      "=== Results ===\n",
      "results are substantially better for forensic image edge preservation and noise removal than the linear and nonlinear filter approaches. Tertychnyi et al. focused on extremely blurry fingerprint images that exhibited a variety of well known aberrations, including dryness, wetness, dot presence, physical damage, and blurriness. The VGG16based deep learning model was used to classify fingerprints and dry fingerprints with accuracies of 84 and 93 respectively. Taee and Abdulsamad  proposed the BRIS...\n",
      "\n",
      "\n",
      "=== Discussion ===\n",
      "This overview encompasses a discussion of various IETs employed in Fingerprint Recognition  applications. A selfassembled dataset of 50,130 fingerprint images from FoD sensing was used to test the approach, which showed that it could achieve 95.83 . The provides an analysis of the performance of recent unimodal fingerprint biometric systems. B. TECHNIQUES FOR FEATURE EXTRACTION AND CLASSIFICATION BASED ON FINGERPRINT PATTERNS The performance of modern automated fingerprint recogni tion systems i...\n",
      "\n",
      "\n",
      "=== Conclusion ===\n",
      "...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section, content in sections.items():\n",
    "    print(f\"\\n=== {section} ===\\n{content[:500]}...\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36cf59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42be90ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "class HybridSummarizer:\n",
    "    def __init__(self, extractive_model='en_core_web_sm', abstractive_model='facebook/bart-large-cnn'):\n",
    "        # Extractive model setup\n",
    "        self.nlp = spacy.load(extractive_model)\n",
    "        \n",
    "        # Abstractive model setup\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(abstractive_model)\n",
    "        self.abstractive_model = AutoModelForSeq2SeqLM.from_pretrained(abstractive_model)\n",
    "        \n",
    "        # Section summarization strategy\n",
    "        self.summarization_strategy = {\n",
    "            'Methods': self.extractive_summary,\n",
    "            'Results': self.extractive_summary,\n",
    "            'Introduction': self.abstractive_summary,\n",
    "            'Discussion': self.abstractive_summary,\n",
    "            'Conclusion': self.abstractive_summary\n",
    "        }\n",
    "\n",
    "    def extractive_summary(self, text, max_sentences=3):\n",
    "        \"\"\"Generate extractive summary using TF-IDF\"\"\"\n",
    "        sentences = [sent.text for sent in self.nlp(text).sents]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "        top_sentence_indices = sorted(\n",
    "            range(len(sentence_scores)), \n",
    "            key=lambda i: sentence_scores[i, 0],  # Fix indexing issue\n",
    "            reverse=True\n",
    "        )[:max_sentences]\n",
    "        \n",
    "        return ' '.join([sentences[i] for i in sorted(top_sentence_indices)])\n",
    "\n",
    "    def abstractive_summary(self, text, max_length=150, min_length=50):\n",
    "        \"\"\"Generate abstractive summary using BART\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            max_length=1024, \n",
    "            return_tensors='pt', \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        summary_ids = self.abstractive_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            num_beams=4, \n",
    "            max_length=max_length, \n",
    "            min_length=min_length,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(\n",
    "            summary_ids[0], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "\n",
    "    def summarize_sections(self, sections):\n",
    "        \"\"\"Generate summaries for different sections using appropriate strategy\"\"\"\n",
    "        summaries = {}\n",
    "        for section, content in sections.items():\n",
    "            if content.strip():\n",
    "                # Select summarization method based on section\n",
    "                summarizer = self.summarization_strategy.get(\n",
    "                    section, \n",
    "                    self.extractive_summary  # Default fallback\n",
    "                )\n",
    "                summaries[section] = summarizer(content)\n",
    "        \n",
    "        return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5328d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_research_paper(pdf_path):\n",
    "    \"\"\"Process and summarize a research paper\"\"\"\n",
    "    # Existing text extraction and cleaning functions\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    sections = split_into_sections(cleaned_text)\n",
    "    \n",
    "    # Initialize hybrid summarizer\n",
    "    summarizer = HybridSummarizer()\n",
    "    \n",
    "    # Generate section summaries\n",
    "    section_summaries = summarizer.summarize_sections(sections)\n",
    "    \n",
    "    # Print summaries\n",
    "    for section, summary in section_summaries.items():\n",
    "        print(f\"{section} Summary:\")\n",
    "        print(summary)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return section_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d9916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"A_Comprehensive_Review_of_Unimodal_and_Multimodal_Fingerprint_Biometric_Authentication_Systems_Fusion_Attacks_and_Template_Protection.pdf\"  \n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "summaries = process_research_paper(pdf_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
