{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f97260f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fitz\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: httplib2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from fitz) (0.22.0)\n",
      "Collecting configobj\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Collecting pyxnat\n",
      "  Downloading pyxnat-1.6.2-py3-none-any.whl (95 kB)\n",
      "     ---------------------------------------- 95.6/95.6 kB 5.7 MB/s eta 0:00:00\n",
      "Collecting nipype\n",
      "  Downloading nipype-1.9.2-py3-none-any.whl (3.2 MB)\n",
      "     ---------------------------------------- 3.2/3.2 MB 14.6 MB/s eta 0:00:00\n",
      "Collecting nibabel\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 17.5 MB/s eta 0:00:00\n",
      "Collecting configparser\n",
      "  Downloading configparser-7.1.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from fitz) (1.3.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from fitz) (1.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from fitz) (1.23.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from httplib2->fitz) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from nibabel->fitz) (4.12.1)\n",
      "Collecting importlib-resources>=5.12\n",
      "  Downloading importlib_resources-6.5.2-py3-none-any.whl (37 kB)\n",
      "Requirement already satisfied: packaging>=20 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from nibabel->fitz) (22.0)\n",
      "Collecting looseversion!=1.2\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Requirement already satisfied: click>=6.6.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from nipype->fitz) (8.0.4)\n",
      "Collecting pydot>=1.2.3\n",
      "  Downloading pydot-3.0.4-py3-none-any.whl (35 kB)\n",
      "Collecting acres\n",
      "  Downloading acres-0.2.0-py3-none-any.whl (10 kB)\n",
      "Collecting rdflib>=5.0.0\n",
      "  Downloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
      "     ------------------------------------- 564.9/564.9 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting puremagic\n",
      "  Downloading puremagic-1.28-py3-none-any.whl (43 kB)\n",
      "     ---------------------------------------- 43.2/43.2 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from nipype->fitz) (2.8.2)\n",
      "Requirement already satisfied: filelock>=3.0.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from nipype->fitz) (3.9.0)\n",
      "Collecting simplejson>=3.8.0\n",
      "  Downloading simplejson-3.19.3-cp310-cp310-win_amd64.whl (75 kB)\n",
      "     ---------------------------------------- 75.6/75.6 kB 4.4 MB/s eta 0:00:00\n",
      "Collecting traits>=6.2\n",
      "  Downloading traits-7.0.2-cp310-cp310-win_amd64.whl (5.0 MB)\n",
      "     ---------------------------------------- 5.0/5.0 MB 17.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.5 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from nipype->fitz) (2.8.4)\n",
      "Collecting etelemetry>=0.3.1\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Collecting prov>=1.5.2\n",
      "  Downloading prov-2.0.1-py3-none-any.whl (421 kB)\n",
      "     -------------------------------------- 421.5/421.5 kB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from pandas->fitz) (2022.7)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (2.32.3)\n",
      "Requirement already satisfied: pathlib>=1.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (1.0.1)\n",
      "Requirement already satisfied: lxml>=4.3 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from pyxnat->fitz) (4.9.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from click>=6.6.0->nipype->fitz) (0.4.6)\n",
      "Collecting ci-info>=0.2\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Collecting rdflib>=5.0.0\n",
      "  Downloading rdflib-6.3.2-py3-none-any.whl (528 kB)\n",
      "     ------------------------------------- 528.1/528.1 kB 11.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from python-dateutil>=2.2->nipype->fitz) (1.16.0)\n",
      "Collecting isodate<0.7.0,>=0.6.0\n",
      "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
      "     ---------------------------------------- 41.7/41.7 kB 2.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests>=2.20->pyxnat->fitz) (3.4)\n",
      "Installing collected packages: puremagic, looseversion, traits, simplejson, pydot, isodate, importlib-resources, configparser, configobj, ci-info, rdflib, pyxnat, nibabel, etelemetry, acres, prov, nipype, fitz\n",
      "Successfully installed acres-0.2.0 ci-info-0.3.0 configobj-5.0.9 configparser-7.1.0 etelemetry-0.3.1 fitz-0.0.1.dev2 importlib-resources-6.5.2 isodate-0.6.1 looseversion-1.3.0 nibabel-5.3.2 nipype-1.9.2 prov-2.0.1 puremagic-1.28 pydot-3.0.4 pyxnat-1.6.2 rdflib-6.3.2 simplejson-3.19.3 traits-7.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install fitz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a83d23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.25.2-cp39-abi3-win_amd64.whl (16.5 MB)\n",
      "     ---------------------------------------- 16.5/16.5 MB 7.5 MB/s eta 0:00:00\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.25.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ed550bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF for PDF text extraction\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fbeaf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sumiran\n",
      "[nltk_data]     Grover\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4184d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts text from a PDF file and removes extra spaces and line breaks.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "\n",
    "    # Remove multiple spaces and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b1503c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"Preprocesses extracted text by removing references, citations, and special characters.\"\"\"\n",
    "    # Remove in-text citations like [1], (Smith et al., 2020)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\d{4}.*?\\)', '', text)\n",
    "\n",
    "    # Remove special characters except basic punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?;:()\\s]', '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f136226e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sections(text):\n",
    "    \"\"\"Splits research paper text into logical sections using common headers.\"\"\"\n",
    "    sections = {\n",
    "        \"Title\": \"\",\n",
    "        \"Abstract\": \"\",\n",
    "        \"Introduction\": \"\",\n",
    "        \"Methods\": \"\",\n",
    "        \"Results\": \"\",\n",
    "        \"Discussion\": \"\",\n",
    "        \"Conclusion\": \"\",\n",
    "    }\n",
    "\n",
    "    sentences = sent_tokenize(text)  # Tokenize into sentences\n",
    "\n",
    "    current_section = \"Title\"\n",
    "    for sentence in sentences:\n",
    "        # Detect section headers\n",
    "        if \"abstract\" in sentence.lower():\n",
    "            current_section = \"Abstract\"\n",
    "        elif \"introduction\" in sentence.lower():\n",
    "            current_section = \"Introduction\"\n",
    "        elif \"method\" in sentence.lower():\n",
    "            current_section = \"Methods\"\n",
    "        elif \"result\" in sentence.lower():\n",
    "            current_section = \"Results\"\n",
    "        elif \"discussion\" in sentence.lower():\n",
    "            current_section = \"Discussion\"\n",
    "        elif \"conclusion\" in sentence.lower():\n",
    "            current_section = \"Conclusion\"\n",
    "\n",
    "        # Append to the appropriate section\n",
    "        sections[current_section] += sentence + \" \"\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c88f46c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = \"A_Comprehensive_Review_of_Unimodal_and_Multimodal_Fingerprint_Biometric_Authentication_Systems_Fusion_Attacks_and_Template_Protection.pdf\"  # Change to your file path\n",
    "raw_text = extract_text_from_pdf(pdf_path)\n",
    "cleaned_text = clean_text(raw_text)\n",
    "sections = split_into_sections(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f783312e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Title ===\n",
      "Received 7 March 2024, accepted 25 April 2024, date of publication 30 April 2024, date of current version 13 May 2024. Digital Object Identifier 10.1109ACCESS.2024.3395417 A Comprehensive Review of Unimodal and Multimodal Fingerprint Biometric Authentication Systems: Fusion, Attacks, and Template Protection U. SUMALATHA 1, K. KRISHNA PRAKASHA 1, , Manipal Academy of Higher Education , Manipal Academy of Higher Education  and Srikanth Prabhu  Universality: Anyone must be able to use the applica t...\n",
      "\n",
      "\n",
      "=== Abstract ===\n",
      "...\n",
      "\n",
      "\n",
      "=== Introduction ===\n",
      "A. K. Jain, A. Ross, and S. Prabhakar, An introduction to biometric recognition, IEEE Trans. Circuits Syst. Video Technol., vol. 14, no. 1, pp. 420, Jan. 2004. A. K. Jain, A. Ross, and K. Nandakumar, Introduction to Biometrics. Cham, Switzerland: Springer, 2016. K. ElMaleh and W. ElHajj, Voice biometrics: Security, forensics, and healthcare, J. Med. Syst., vol. 43, no. 9, p. 306, 2019. K. Banerjee, J. P. Singh, and R. Kaur, Human retinal identification: Review and future scope, J. Comput. Sci. T...\n",
      "\n",
      "\n",
      "=== Methods ===\n",
      "used BSplines and presented a method that is both quicker and more effective at eliminating impulsive noise while maintaining the images edges. The 64308 VOLUME 12, 2024 U. Sumalatha et al. : Unimodal and Multimodal Fingerprint Biometric Authentication Systems TABLE 4. Modes of fingerprint data acquisition. TABLE 5. Fingerprint datasets overview. The Table 7 and Table 8 describes fingerprint feature extraction traditional and deep learning methods. Fingerprint classification has evolved since th...\n",
      "\n",
      "\n",
      "=== Results ===\n",
      "2) Nonuniversality: A segment of the population may be unable or unwilling to give the requisite biometric feature precisely, resulting in an increased failure to enroll rate  With the help of multibiometric systems, enrollment phase problems such as nonuniversality are resolved, allowing for adequate population coverage. As a result, even if a user cannot provide one biometric trait, they can still enroll and be recognized by providing a different biometric trait. For instance, despite having p...\n",
      "\n",
      "\n",
      "=== Discussion ===\n",
      "This overview encompasses a discussion of various IETs employed in Fingerprint Recognition  applications. A selfassembled dataset of 50,130 fingerprint images from FoD sensing was used to test the approach, which showed that it could achieve 95.83 . The Table 6 provides an analysis of the performance of recent unimodal fingerprint biometric systems. B. TECHNIQUES FOR FEATURE EXTRACTION AND CLASSIFICATION BASED ON FINGERPRINT PATTERNS The performance of modern automated fingerprint recogni tion s...\n",
      "\n",
      "\n",
      "=== Conclusion ===\n",
      "CONCLUSION We discussed the benefits and drawbacks of fingerprint recognition systems by performing a performance analysis of each type of system. This review focuses on and studies the considerable advancements in fingerprint biometrics, both unimodal and multimodal. As the benefits of fingerprint biometric systems are being discussed, several application scenarios illustrating the algorithms used to construct fin gerprint biometric systems are emphasized. We found that although some devices th...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for section, content in sections.items():\n",
    "    print(f\"\\n=== {section} ===\\n{content[:500]}...\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23009a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4934125d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (4.24.0)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "     ---------------------------------------- 9.7/9.7 MB 9.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from transformers) (4.67.0)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-win_amd64.whl (303 kB)\n",
      "     -------------------------------------- 303.8/303.8 kB 6.2 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.22,>=0.21\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-win_amd64.whl (2.4 MB)\n",
      "     ---------------------------------------- 2.4/2.4 MB 13.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Installing collected packages: safetensors, tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.11.4\n",
      "    Uninstalling tokenizers-0.11.4:\n",
      "      Successfully uninstalled tokenizers-0.11.4\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.24.0\n",
      "    Uninstalling transformers-4.24.0:\n",
      "      Successfully uninstalled transformers-4.24.0\n",
      "Successfully installed safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.2\n",
      "Requirement already satisfied: tensorflow in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.12.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.66.2)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.12.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.9.2)\n",
      "Requirement already satisfied: optree in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.13.0)\n",
      "Requirement already satisfied: namex in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.2)\n",
      "Requirement already satisfied: keras in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (22.0)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (0.4.1)\n",
      "Requirement already satisfied: rich in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (13.9.2)\n",
      "Requirement already satisfied: optree in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (0.13.0)\n",
      "Requirement already satisfied: absl-py in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (1.23.5)\n",
      "Requirement already satisfied: h5py in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: namex in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from optree->keras) (4.12.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sumiran grover\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install tensorflow\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bc6c6a7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class ResearchSummarizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def extract_summary(self, text, max_sentences=3):\n",
    "        \"\"\"Generate extractive summary using TF-IDF\"\"\"\n",
    "        sentences = [sent.text for sent in self.nlp(text).sents]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "        top_sentence_indices = sorted(\n",
    "            range(len(sentence_scores)), \n",
    "            key=lambda i: sentence_scores[i], \n",
    "            reverse=True\n",
    "        )[:max_sentences]\n",
    "        \n",
    "        return ' '.join([sentences[i] for i in sorted(top_sentence_indices)])\n",
    "    \n",
    "    def extract_keywords(self, text, top_k=5):\n",
    "        \"\"\"Extract top keywords using TF-IDF\"\"\"\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform([text])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        sorted_indices = tfidf_matrix.toarray()[0].argsort()[::-1]\n",
    "        return [feature_names[i] for i in sorted_indices[:top_k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8ddb601",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m summary_results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_summarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msections\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[19], line 6\u001b[0m, in \u001b[0;36mevaluate_summarizer\u001b[1;34m(sections)\u001b[0m\n\u001b[0;32m      4\u001b[0m results \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m section, content \u001b[38;5;129;01min\u001b[39;00m sections\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m----> 6\u001b[0m     summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m summarizer\u001b[38;5;241m.\u001b[39mextract_keywords(content)\n\u001b[0;32m      9\u001b[0m     results[section] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     10\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary_length\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(summary\u001b[38;5;241m.\u001b[39msplit()),\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m: summary,\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m: keywords\n\u001b[0;32m     13\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[18], line 13\u001b[0m, in \u001b[0;36mResearchSummarizer.extract_summary\u001b[1;34m(self, text, max_sentences)\u001b[0m\n\u001b[0;32m     10\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [sent\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnlp(text)\u001b[38;5;241m.\u001b[39msents]\n\u001b[0;32m     12\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m sentence_scores \u001b[38;5;241m=\u001b[39m tfidf_matrix\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m top_sentence_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence_scores)), \n\u001b[0;32m     18\u001b[0m     key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m i: sentence_scores[i], \n\u001b[0;32m     19\u001b[0m     reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     20\u001b[0m )[:max_sentences]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2138\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2131\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_params()\n\u001b[0;32m   2132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf \u001b[38;5;241m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2133\u001b[0m     norm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm,\n\u001b[0;32m   2134\u001b[0m     use_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_idf,\n\u001b[0;32m   2135\u001b[0m     smooth_idf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msmooth_idf,\n\u001b[0;32m   2136\u001b[0m     sublinear_tf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msublinear_tf,\n\u001b[0;32m   2137\u001b[0m )\n\u001b[1;32m-> 2138\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tfidf\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m   2140\u001b[0m \u001b[38;5;66;03m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2141\u001b[0m \u001b[38;5;66;03m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1389\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1381\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1382\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1383\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1384\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1385\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1386\u001b[0m             )\n\u001b[0;32m   1387\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1389\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1392\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1295\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1293\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1295\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1296\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1297\u001b[0m         )\n\u001b[0;32m   1299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1300\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "def evaluate_summarizer(sections):\n",
    "    summarizer = ResearchSummarizer()\n",
    "    \n",
    "    results = {}\n",
    "    for section, content in sections.items():\n",
    "        summary = summarizer.extract_summary(content)\n",
    "        keywords = summarizer.extract_keywords(content)\n",
    "        \n",
    "        results[section] = {\n",
    "            'summary_length': len(summary.split()),\n",
    "            'summary': summary,\n",
    "            'keywords': keywords\n",
    "        }\n",
    "    \n",
    "    # Simple quality metrics\n",
    "    print(\"Summary Quality Metrics:\")\n",
    "    for section, data in results.items():\n",
    "        print(f\"{section}:\")\n",
    "        print(f\"  Summary Length: {data['summary_length']} words\")\n",
    "        print(f\"  Keywords: {data['keywords']}\")\n",
    "        print(f\"  Summary: {data['summary']}\\n\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run evaluation\n",
    "summary_results = evaluate_summarizer(sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3013e93a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TITLE:\n",
      "Summary: Digital Object Identifier 10.1109ACCESS.2024.3395417 A Comprehensive Review of Unimodal and Multimodal Fingerprint Biometric Authentication Systems: Fusion, Attacks, and Template Protection U. SUMALATHA 1, K. KRISHNA PRAKASHA 1, , Manipal Academy of Higher Education , Manipal Academy of Higher Education  and Srikanth Prabhu  Universality: Anyone must be able to use the applica tion using biometric features. Performance: The overall accuracy should be high, with low False Acceptance Rate  illumination due to its sensitivity to light . Similarly, palmprints boast reliability and distinctiveness through ridge patterns, principal lines, minutiae details, delta points, and complex textures , while knuckleprints offer discriminating features captured by various sensors .\n",
      "Keywords: ['biometric', '2024', 'accuracy', 'fingerprint', 'traits']\n",
      "\n",
      "INTRODUCTION:\n",
      "Summary: A. B. A. Hassanat, V. B. S. Prasath, M. Alkasassbeh, A. S. Tarawneh, and A. J. Alshamailh, Magnetic energybased feature extraction for low quality fingerprint images, Signal, Image Video Process., vol. 12, no. 8, pp. 14711478, Nov. 2018. M. Sood and A. Girdhar, A novel approach for lowquality fingerprint image enhancement using spatial and frequency domain filtering techniques, in Cognitive Behavior and Human Computer Interaction Based on Machine Learning Algorithm, 2021, pp. Z. Zhang, X. Zhao, X. Zhang, X. Hou, X. Ma, S. Tang, Y. Zhang, G. Xu, Q. Liu, and S. Long, Insensor reservoir computing system for latent fingerprint recognition with deep ultraviolet photosynapses and memristor array, Nature Commun., vol. 13, no. 1, p. 6590, Nov. 2022.\n",
      "Keywords: ['pp', 'vol', 'fingerprint', 'using', 'recognition']\n",
      "\n",
      "METHODS:\n",
      "Summary: Recent advancements in deep learning and error correction coding enhance security in multibiometric sys tems , with researchers exploring fusion architectures and integrating secure sketch cancelable blocks  hashing, developed by Jin et al. , converts realvalue biometric feature vectors into discrete indexed hashed codes, yielding an EER of 4.10 in the stolen token scenario. Additionally, Ghammam et al. introduced a transformation function effective against specific attacks, embedding biometric data into an orthogonalized pseudo random numeric matrix created using a secret key or token and nonlinear operations, with an EER of 0.4 at a GAR of 99. Poonguzhali and Ezhilarasan  improved a unibio metric fingerprint recognition system by integrating feature levels at Levels 1 and 2, finding concatenated feature sets more effective than discrete ones, particularly with their Fin gerprint Feature Vector approach leveraging richer gray level data and analyzing poorquality images.\n",
      "Keywords: ['fingerprint', 'biometric', 'pp', 'vol', 'multimodal']\n",
      "\n",
      "RESULTS:\n",
      "Summary: 2) Nonuniversality: A segment of the population may be unable or unwilling to give the requisite biometric feature precisely, resulting in an increased failure to enroll rate  With the help of multibiometric systems, enrollment phase problems such as nonuniversality are resolved, allowing for adequate population coverage. Tertychnyi et al. focused on extremely blurry fingerprint images that exhibited a variety of well known aberrations, including dryness, wetness, dot presence, physical damage, and blurriness. NS et al. observed that while the accuracy rate of an electrocardiogram  and QGaussian Multiple Support Vector Machines  was employed to fully define the orientation and position interactions between neighboring pixels .\n",
      "Keywords: ['fingerprint', 'biometric', 'al', 'et', 'using']\n",
      "\n",
      "DISCUSSION:\n",
      "Summary: A selfassembled dataset of 50,130 fingerprint images from FoD sensing was used to test the approach, which showed that it could achieve 95.83 . At Level 2, features are associated with minutiae infor mation, including ridge bifurcations and endings, making each fingerprint a unique pattern. At Level 3, features encompass ridge dimensional attributes like ridge path deviation, width, shape, sweat pore locations, geometric details of the ridges, and edge contours.\n",
      "Keywords: ['fingerprint', 'ridge', 'level', 'extraction', 'features']\n",
      "\n",
      "CONCLUSION:\n",
      "Summary: As the benefits of fingerprint biometric systems are being discussed, several application scenarios illustrating the algorithms used to construct fin gerprint biometric systems are emphasized. We found that although some devices that use dynamic biometrics need to improve their verification accuracy, most of the current solutions have privacy and security flaws. Multimodal biometric systems are to be improved by incorporating additional sensors, improving matching algorithms, handling noise errors, and analyzing data.\n",
      "Keywords: ['systems', 'biometric', 'recognition', 'fingerprint', 'biometrics']\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "import re\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = \"\"\n",
    "    for page in doc:\n",
    "        text += page.get_text(\"text\") + \"\\n\"\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    text = re.sub(r'\\(.*?\\d{4}.*?\\)', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?;:()\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def split_into_sections(text):\n",
    "    sections = {\n",
    "        \"Title\": \"\",\n",
    "        \"Abstract\": \"\",\n",
    "        \"Introduction\": \"\",\n",
    "        \"Methods\": \"\",\n",
    "        \"Results\": \"\",\n",
    "        \"Discussion\": \"\",\n",
    "        \"Conclusion\": \"\",\n",
    "    }\n",
    "    sentences = sent_tokenize(text)\n",
    "    current_section = \"Title\"\n",
    "    for sentence in sentences:\n",
    "        if \"abstract\" in sentence.lower():\n",
    "            current_section = \"Abstract\"\n",
    "        elif \"introduction\" in sentence.lower():\n",
    "            current_section = \"Introduction\"\n",
    "        elif \"method\" in sentence.lower():\n",
    "            current_section = \"Methods\"\n",
    "        elif \"result\" in sentence.lower():\n",
    "            current_section = \"Results\"\n",
    "        elif \"discussion\" in sentence.lower():\n",
    "            current_section = \"Discussion\"\n",
    "        elif \"conclusion\" in sentence.lower():\n",
    "            current_section = \"Conclusion\"\n",
    "        sections[current_section] += sentence + \" \"\n",
    "    return sections\n",
    "\n",
    "class ResearchSummarizer:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "    \n",
    "    def extract_summary(self, text, max_sentences=3):\n",
    "        sentences = [sent.text for sent in self.nlp(text).sents]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "        top_sentence_indices = sorted(\n",
    "            range(len(sentence_scores)), \n",
    "            key=lambda i: sentence_scores[i], \n",
    "            reverse=True\n",
    "        )[:max_sentences]\n",
    "        \n",
    "        return ' '.join([sentences[i] for i in sorted(top_sentence_indices)])\n",
    "    \n",
    "    def extract_keywords(self, text, top_k=5):\n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform([text])\n",
    "        feature_names = vectorizer.get_feature_names_out()\n",
    "        \n",
    "        sorted_indices = tfidf_matrix.toarray()[0].argsort()[::-1]\n",
    "        return [feature_names[i] for i in sorted_indices[:top_k]]\n",
    "\n",
    "def analyze_paper(pdf_path):\n",
    "    # Extract and process text\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    sections = split_into_sections(cleaned_text)\n",
    "    \n",
    "    # Initialize summarizer\n",
    "    summarizer = ResearchSummarizer()\n",
    "    \n",
    "    # Generate insights for each section\n",
    "    insights = {}\n",
    "    for section, content in sections.items():\n",
    "        if content.strip():\n",
    "            insights[section] = {\n",
    "                'summary': summarizer.extract_summary(content),\n",
    "                'keywords': summarizer.extract_keywords(content)\n",
    "            }\n",
    "    \n",
    "    # Print results\n",
    "    for section, data in insights.items():\n",
    "        print(f\"\\n{section.upper()}:\")\n",
    "        print(\"Summary:\", data['summary'])\n",
    "        print(\"Keywords:\", data['keywords'])\n",
    "    \n",
    "    return insights\n",
    "\n",
    "# Usage\n",
    "pdf_path = \"A_Comprehensive_Review_of_Unimodal_and_Multimodal_Fingerprint_Biometric_Authentication_Systems_Fusion_Attacks_and_Template_Protection.pdf\"\n",
    "paper_insights = analyze_paper(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4793ce75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM,  # For abstractive summarization\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "45c0a2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40201e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridSummarizer:\n",
    "    def __init__(self, \n",
    "                 extractive_model='en_core_web_sm', \n",
    "                 abstractive_model='facebook/bart-large-cnn'):\n",
    "        # Extractive model setup\n",
    "        self.nlp = spacy.load(extractive_model)\n",
    "        \n",
    "        # Abstractive model setup\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(abstractive_model)\n",
    "        self.abstractive_model = AutoModelForSeq2SeqLM.from_pretrained(abstractive_model)\n",
    "        \n",
    "        # Section summarization strategy\n",
    "        self.summarization_strategy = {\n",
    "            'Methods': self.extractive_summary,\n",
    "            'Results': self.extractive_summary,\n",
    "            'Introduction': self.abstractive_summary,\n",
    "            'Discussion': self.abstractive_summary,\n",
    "            'Conclusion': self.abstractive_summary\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47f394c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractive_summary(self, text, max_sentences=3):\n",
    "        \"\"\"Generate extractive summary using TF-IDF\"\"\"\n",
    "        sentences = [sent.text for sent in self.nlp(text).sents]\n",
    "        \n",
    "        vectorizer = TfidfVectorizer(stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "        \n",
    "        sentence_scores = tfidf_matrix.sum(axis=1)\n",
    "        top_sentence_indices = sorted(\n",
    "            range(len(sentence_scores)), \n",
    "            key=lambda i: sentence_scores[i], \n",
    "            reverse=True\n",
    "        )[:max_sentences]\n",
    "        \n",
    "        return ' '.join([sentences[i] for i in sorted(top_sentence_indices)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa41f965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstractive_summary(self, text, max_length=150, min_length=50):\n",
    "        \"\"\"Generate abstractive summary using BART\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text, \n",
    "            max_length=1024, \n",
    "            return_tensors='pt', \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        summary_ids = self.abstractive_model.generate(\n",
    "            inputs['input_ids'], \n",
    "            num_beams=4, \n",
    "            max_length=max_length, \n",
    "            min_length=min_length,\n",
    "            early_stopping=True\n",
    "        )\n",
    "        \n",
    "        return self.tokenizer.decode(\n",
    "            summary_ids[0], \n",
    "            skip_special_tokens=True\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7385f3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    " def summarize_sections(self, sections):\n",
    "        \"\"\"\n",
    "        Generate summaries for different sections using \n",
    "        appropriate summarization strategy\n",
    "        \"\"\"\n",
    "        summaries = {}\n",
    "        for section, content in sections.items():\n",
    "            if content.strip():\n",
    "                # Select summarization method based on section\n",
    "                summarizer = self.summarization_strategy.get(\n",
    "                    section, \n",
    "                    self.extractive_summary  # Default fallback\n",
    "                )\n",
    "                summaries[section] = summarizer(content)\n",
    "        \n",
    "        return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30c46a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_research_paper(pdf_path):\n",
    "    # Existing text extraction and cleaning functions\n",
    "    raw_text = extract_text_from_pdf(pdf_path)\n",
    "    cleaned_text = clean_text(raw_text)\n",
    "    sections = split_into_sections(cleaned_text)\n",
    "    \n",
    "    # Initialize hybrid summarizer\n",
    "    summarizer = HybridSummarizer()\n",
    "    \n",
    "    # Generate section summaries\n",
    "    section_summaries = summarizer.summarize_sections(sections)\n",
    "    \n",
    "    # Print or further process summaries\n",
    "    for section, summary in section_summaries.items():\n",
    "        print(f\"{section} Summary:\")\n",
    "        print(summary)\n",
    "        print(\"-\" * 50)\n",
    "    \n",
    "    return section_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "efef741f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd5485df458d49ddb52524047cfd6e72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Sumiran Grover\\.cache\\huggingface\\hub\\models--facebook--bart-large-cnn. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec5d2a035d04965a809909650654fda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b844e6a7c07484390191b4155df754c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b501b124924202926f9b7327460161",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1076, in _get_module\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\importlib\\__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"<frozen importlib._bootstrap>\", line 1050, in _gcd_import\n",
      "  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 1004, in _find_and_load_unlocked\n",
      "ModuleNotFoundError: No module named 'transformers.models.xlm_prophetnet.configuration_xlm_prophetnet'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\Sumiran Grover\\AppData\\Local\\Temp\\ipykernel_47172\\2416997931.py\", line 2, in <module>\n",
      "    summaries = process_research_paper(pdf_path)\n",
      "  File \"C:\\Users\\Sumiran Grover\\AppData\\Local\\Temp\\ipykernel_47172\\2607567253.py\", line 8, in process_research_paper\n",
      "    summarizer = HybridSummarizer()\n",
      "  File \"C:\\Users\\Sumiran Grover\\AppData\\Local\\Temp\\ipykernel_47172\\3812251116.py\", line 10, in __init__\n",
      "    self.abstractive_model = AutoModelForSeq2SeqLM.from_pretrained(abstractive_model)\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 461, in from_pretrained\n",
      "    \"token\",\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 607, in keys\n",
      "    raise ValueError(\"Cannot specify `output_loading_info=True` when loading from timm\")\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 608, in <listcomp>\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 604, in _load_attr_from_module\n",
      "    raise ValueError(\"Cannot specify `out_features` for timm backbones\")\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 553, in getattribute_from_module\n",
      "    model_class = get_class_from_dynamic_module(\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1066, in __getattr__\n",
      "    package_version = importlib.metadata.version(\"Pillow-SIMD\")\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py\", line 1078, in _get_module\n",
      "    return _pytest_available\n",
      "RuntimeError: Failed to import transformers.models.xlm_prophetnet.configuration_xlm_prophetnet because of the following error (look up to see its traceback):\n",
      "No module named 'transformers.models.xlm_prophetnet.configuration_xlm_prophetnet'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1118, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1012, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 865, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 818, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(r))\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 736, in format_record\n",
      "    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 698, in lines\n",
      "    pieces = self.included_pieces\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 649, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\stack_data\\utils.py\", line 145, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\stack_data\\core.py\", line 628, in executing_piece\n",
      "    return only(\n",
      "  File \"C:\\Users\\Sumiran Grover\\anaconda3\\lib\\site-packages\\executing\\executing.py\", line 164, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "pdf_path = \"A_Comprehensive_Review_of_Unimodal_and_Multimodal_Fingerprint_Biometric_Authentication_Systems_Fusion_Attacks_and_Template_Protection.pdf\"\n",
    "summaries = process_research_paper(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3a28c1e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class AdvancedResearchSummarizer:\n",
    "    def extractive_summary(self, text, max_sentences=3):\n",
    "        # Enhance sentence selection criteria\n",
    "        sentences = list(self.nlp(text).sents)\n",
    "        \n",
    "        # Score sentences based on multiple factors\n",
    "        def sentence_score(sentence):\n",
    "            # Linguistic features\n",
    "            noun_chunks = len(list(sentence.noun_chunks))\n",
    "            named_entities = len(list(sentence.ents))\n",
    "            \n",
    "            # TF-IDF scoring\n",
    "            tfidf_score = self.vectorizer.transform([sentence.text]).sum()\n",
    "            \n",
    "            # Combine scoring mechanisms\n",
    "            return (\n",
    "                noun_chunks * 0.3 + \n",
    "                named_entities * 0.2 + \n",
    "                tfidf_score * 0.5\n",
    "            )\n",
    "        \n",
    "        # Rank sentences\n",
    "        scored_sentences = sorted(\n",
    "            sentences, \n",
    "            key=sentence_score, \n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Select top sentences maintaining original order\n",
    "        selected_sentences = sorted(\n",
    "            scored_sentences[:max_sentences], \n",
    "            key=lambda s: text.index(s.text)\n",
    "        )\n",
    "        \n",
    "        return ' '.join([str(sent) for sent in selected_sentences])\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a376f95b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conclusion Summary:\n",
      "As the benefits of fingerprint biometric systems are being discussed, several application scenarios illustrating the algorithms used to construct fin gerprint biometric systems are emphasized. We found that although some devices that use dynamic biometrics need to improve their verification accuracy, most of the current solutions have privacy and security flaws. Of all the levels of data fusion, score level fusion continues to be the most beneficial because it is simple to identify and merge the matching scores. Multimodal biometric systems are to be improved by incorporating additional sensors, improving matching algorithms, handling noise errors, and analyzing data. S. Dargan and M. Kumar, A comprehensive survey on the biometric recognition systems based on physiological and behavioral modalities, Expert Syst.\n",
      "--------------------------------------------------\n",
      "Introduction Summary:\n",
      "C. Kant and S. Chaudhary, A multimodal biometric system based on finger knuckle print, fingerprint, and palmprint traits, in Innovations in Computational Intelligence and Computer Vision, M. K. Sharma, V. S. Dhaka, T. Perumal, N. Dey, and J. M. R. S. Tavares, Eds. A. B. A. Hassanat, V. B. S. Prasath, M. Alkasassbeh, A. S. Tarawneh, and A. J. Alshamailh, Magnetic energybased feature extraction for low quality fingerprint images, Signal, Image Video Process., vol. 12, no. 8, pp. 14711478, Nov. 2018. M. Sood and A. Girdhar, A novel approach for lowquality fingerprint image enhancement using spatial and frequency domain filtering techniques, in Cognitive Behavior and Human Computer Interaction Based on Machine Learning Algorithm, 2021, pp. Z. Zhang, X. Zhao, X. Zhang, X. Hou, X. Ma, S. Tang, Y. Zhang, G. Xu, Q. Liu, and S. Long, Insensor reservoir computing system for latent fingerprint recognition with deep ultraviolet photosynapses and memristor array, Nature Commun., vol. 13, no. 1, p. 6590, Nov. 2022.\n",
      "--------------------------------------------------\n",
      "Discussion Summary:\n",
      "A selfassembled dataset of 50,130 fingerprint images from FoD sensing was used to test the approach, which showed that it could achieve 95.83 . The performance of modern automated fingerprint recogni tion systems is significantly affected by feature extraction algorithm. At Level 2, features are associated with minutiae infor mation, including ridge bifurcations and endings, making each fingerprint a unique pattern. At Level 3, features encompass ridge dimensional attributes like ridge path deviation, width, shape, sweat pore locations, geometric details of the ridges, and edge contours.\n",
      "--------------------------------------------------\n",
      "Methods Summary:\n",
      "Recent advancements in deep learning and error correction coding enhance security in multibiometric sys tems , with researchers exploring fusion architectures and integrating secure sketch cancelable blocks  hashing, developed by Jin et al. , converts realvalue biometric feature vectors into discrete indexed hashed codes, yielding an EER of 4.10 in the stolen token scenario. Additionally, Ghammam et al. introduced a transformation function effective against specific attacks, embedding biometric data into an orthogonalized pseudo random numeric matrix created using a secret key or token and nonlinear operations, with an EER of 0.4 at a GAR of 99. Poonguzhali and Ezhilarasan  improved a unibio metric fingerprint recognition system by integrating feature levels at Levels 1 and 2, finding concatenated feature sets more effective than discrete ones, particularly with their Fin gerprint Feature Vector approach leveraging richer gray level data and analyzing poorquality images.\n",
      "--------------------------------------------------\n",
      "Results Summary:\n",
      "2) Nonuniversality: A segment of the population may be unable or unwilling to give the requisite biometric feature precisely, resulting in an increased failure to enroll rate  With the help of multibiometric systems, enrollment phase problems such as nonuniversality are resolved, allowing for adequate population coverage. Tertychnyi et al. focused on extremely blurry fingerprint images that exhibited a variety of well known aberrations, including dryness, wetness, dot presence, physical damage, and blurriness. NS et al. observed that while the accuracy rate of an electrocardiogram  and QGaussian Multiple Support Vector Machines  was employed to fully define the orientation and position interactions between neighboring pixels .\n",
      "--------------------------------------------------\n",
      "Title Summary:\n",
      "Digital Object Identifier 10.1109ACCESS.2024.3395417 A Comprehensive Review of Unimodal and Multimodal Fingerprint Biometric Authentication Systems: Fusion, Attacks, and Template Protection U. SUMALATHA 1, K. KRISHNA PRAKASHA 1, , Manipal Academy of Higher Education , Manipal Academy of Higher Education  and Srikanth Prabhu  Universality: Anyone must be able to use the applica tion using biometric features. Performance: The overall accuracy should be high, with low False Acceptance Rate  illumination due to its sensitivity to light . Similarly, palmprints boast reliability and distinctiveness through ridge patterns, principal lines, minutiae details, delta points, and complex textures , while knuckleprints offer discriminating features captured by various sensors .\n",
      "--------------------------------------------------\n",
      "Conclusion Summary:\n",
      "As the benefits of fingerprint biometric systems are being discussed, several application scenarios illustrating the algorithms used to construct fin gerprint biometric systems are emphasized. We found that although some devices that use dynamic biometrics need to improve their verification accuracy, most of the current solutions have privacy and security flaws. Of all the levels of data fusion, score level fusion continues to be the most beneficial because it is simple to identify and merge the matching scores. Multimodal biometric systems are to be improved by incorporating additional sensors, improving matching algorithms, handling noise errors, and analyzing data. S. Dargan and M. Kumar, A comprehensive survey on the biometric recognition systems based on physiological and behavioral modalities, Expert Syst.\n",
      "--------------------------------------------------\n",
      "Introduction Summary:\n",
      "C. Kant and S. Chaudhary, A multimodal biometric system based on finger knuckle print, fingerprint, and palmprint traits, in Innovations in Computational Intelligence and Computer Vision, M. K. Sharma, V. S. Dhaka, T. Perumal, N. Dey, and J. M. R. S. Tavares, Eds. A. B. A. Hassanat, V. B. S. Prasath, M. Alkasassbeh, A. S. Tarawneh, and A. J. Alshamailh, Magnetic energybased feature extraction for low quality fingerprint images, Signal, Image Video Process., vol. 12, no. 8, pp. 14711478, Nov. 2018. M. Sood and A. Girdhar, A novel approach for lowquality fingerprint image enhancement using spatial and frequency domain filtering techniques, in Cognitive Behavior and Human Computer Interaction Based on Machine Learning Algorithm, 2021, pp. Z. Zhang, X. Zhao, X. Zhang, X. Hou, X. Ma, S. Tang, Y. Zhang, G. Xu, Q. Liu, and S. Long, Insensor reservoir computing system for latent fingerprint recognition with deep ultraviolet photosynapses and memristor array, Nature Commun., vol. 13, no. 1, p. 6590, Nov. 2022.\n",
      "--------------------------------------------------\n",
      "Discussion Summary:\n",
      "A selfassembled dataset of 50,130 fingerprint images from FoD sensing was used to test the approach, which showed that it could achieve 95.83 . The performance of modern automated fingerprint recogni tion systems is significantly affected by feature extraction algorithm. At Level 2, features are associated with minutiae infor mation, including ridge bifurcations and endings, making each fingerprint a unique pattern. At Level 3, features encompass ridge dimensional attributes like ridge path deviation, width, shape, sweat pore locations, geometric details of the ridges, and edge contours.\n",
      "--------------------------------------------------\n",
      "Methods Summary:\n",
      "Recent advancements in deep learning and error correction coding enhance security in multibiometric sys tems , with researchers exploring fusion architectures and integrating secure sketch cancelable blocks  hashing, developed by Jin et al. , converts realvalue biometric feature vectors into discrete indexed hashed codes, yielding an EER of 4.10 in the stolen token scenario. Additionally, Ghammam et al. introduced a transformation function effective against specific attacks, embedding biometric data into an orthogonalized pseudo random numeric matrix created using a secret key or token and nonlinear operations, with an EER of 0.4 at a GAR of 99. Poonguzhali and Ezhilarasan  improved a unibio metric fingerprint recognition system by integrating feature levels at Levels 1 and 2, finding concatenated feature sets more effective than discrete ones, particularly with their Fin gerprint Feature Vector approach leveraging richer gray level data and analyzing poorquality images.\n",
      "--------------------------------------------------\n",
      "Results Summary:\n",
      "2) Nonuniversality: A segment of the population may be unable or unwilling to give the requisite biometric feature precisely, resulting in an increased failure to enroll rate  With the help of multibiometric systems, enrollment phase problems such as nonuniversality are resolved, allowing for adequate population coverage. Tertychnyi et al. focused on extremely blurry fingerprint images that exhibited a variety of well known aberrations, including dryness, wetness, dot presence, physical damage, and blurriness. NS et al. observed that while the accuracy rate of an electrocardiogram  and QGaussian Multiple Support Vector Machines  was employed to fully define the orientation and position interactions between neighboring pixels .\n",
      "--------------------------------------------------\n",
      "Title Summary:\n",
      "Digital Object Identifier 10.1109ACCESS.2024.3395417 A Comprehensive Review of Unimodal and Multimodal Fingerprint Biometric Authentication Systems: Fusion, Attacks, and Template Protection U. SUMALATHA 1, K. KRISHNA PRAKASHA 1, , Manipal Academy of Higher Education , Manipal Academy of Higher Education  and Srikanth Prabhu  Universality: Anyone must be able to use the applica tion using biometric features. Performance: The overall accuracy should be high, with low False Acceptance Rate  illumination due to its sensitivity to light . Similarly, palmprints boast reliability and distinctiveness through ridge patterns, principal lines, minutiae details, delta points, and complex textures , while knuckleprints offer discriminating features captured by various sensors .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Main execution\n",
    "pdf_path = \"A_Comprehensive_Review_of_Unimodal_and_Multimodal_Fingerprint_Biometric_Authentication_Systems_Fusion_Attacks_and_Template_Protection.pdf\"\n",
    "summaries = process_research_paper(pdf_path)\n",
    "summaries = process_research_paper(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a7f59bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_summarization(sections):\n",
    "    summarizer = AdvancedResearchSummarizer()\n",
    "    refined_summaries = {}\n",
    "    \n",
    "    # Section-specific processing\n",
    "    section_priorities = {\n",
    "        'Abstract': 5,   # Most important\n",
    "        'Conclusion': 4, # High priority\n",
    "        'Introduction': 3,\n",
    "        'Discussion': 3,\n",
    "        'Results': 2,\n",
    "        'Methods': 2,\n",
    "        'Title': 1       # Least priority\n",
    "    }\n",
    "    \n",
    "    for section, content in sections.items():\n",
    "        # Dynamic sentence count based on section importance\n",
    "        max_sentences = section_priorities.get(section, 2)\n",
    "        refined_summaries[section] = summarizer.extractive_summary(\n",
    "            content, \n",
    "            max_sentences=max_sentences\n",
    "        )\n",
    "    \n",
    "    return refined_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b464f835",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "import spacy\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86b9d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ResearchPaperProcessor:\n",
    "    def __init__(self):\n",
    "        # Download essential NLP resources\n",
    "        nltk.download('punkt', quiet=True)\n",
    "        \n",
    "        # Load scientific NLP model\n",
    "        self.nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "    def extract_text_from_pdf(self, pdf_path):\n",
    "        \"\"\"Extract raw text from PDF\"\"\"\n",
    "        try:\n",
    "            with open(pdf_path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                text = \" \".join(page.extract_text() for page in reader.pages)\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting PDF text: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        text = text.lower()\n",
    "        doc = self.nlp(text)\n",
    "        cleaned_tokens = [\n",
    "            token.text for token in doc \n",
    "            if not token.is_stop and not token.is_punct\n",
    "        ]\n",
    "        return ' '.join(cleaned_tokens)\n",
    "\n",
    "    def sentence_segmentation(self, text):\n",
    "        \"\"\"Segment text into sentences\"\"\"\n",
    "        return nltk.sent_tokenize(text)\n",
    "\n",
    "    def generate_extractive_summary(self, text, max_sentences=5):\n",
    "        \"\"\"\n",
    "        Generate extractive summary using TF-IDF\n",
    "        \n",
    "        Handles matrix operations more robustly\n",
    "        \"\"\"\n",
    "        sentences = self.sentence_segmentation(text)\n",
    "        \n",
    "        if len(sentences) <= max_sentences:\n",
    "            return sentences\n",
    "        \n",
    "        # TF-IDF Vectorization\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "            \n",
    "            # Convert matrix to dense numpy array for calculations\n",
    "            dense_matrix = tfidf_matrix.toarray()\n",
    "            \n",
    "            # Calculate centroid\n",
    "            centroid = np.mean(dense_matrix, axis=0)\n",
    "            \n",
    "            # Calculate sentence scores\n",
    "            sentence_scores = [\n",
    "                1 - cosine(sent_vec, centroid) \n",
    "                for sent_vec in dense_matrix\n",
    "            ]\n",
    "            \n",
    "            # Select top sentences\n",
    "            top_sentence_indices = sorted(\n",
    "                range(len(sentence_scores)), \n",
    "                key=lambda i: sentence_scores[i], \n",
    "                reverse=True\n",
    "            )[:max_sentences]\n",
    "            \n",
    "            return [sentences[i] for i in top_sentence_indices]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in summary generation: {e}\")\n",
    "            # Fallback: return first max_sentences\n",
    "            return sentences[:max_sentences]\n",
    "\n",
    "    def morphological_analysis(self, text):\n",
    "        \"\"\"Perform morphological analysis\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        \n",
    "        morphology_insights = {\n",
    "            'pos_distribution': {},\n",
    "            'named_entities': [],\n",
    "            'dependency_relations': {}\n",
    "        }\n",
    "        \n",
    "        # Part of Speech Distribution\n",
    "        for token in doc:\n",
    "            pos = token.pos_\n",
    "            morphology_insights['pos_distribution'][pos] = \\\n",
    "                morphology_insights['pos_distribution'].get(pos, 0) + 1\n",
    "        \n",
    "        # Named Entities\n",
    "        morphology_insights['named_entities'] = [\n",
    "            (ent.text, ent.label_) for ent in doc.ents\n",
    "        ]\n",
    "        \n",
    "        # Dependency Relations\n",
    "        for token in doc:\n",
    "            if token.dep_ not in morphology_insights['dependency_relations']:\n",
    "                morphology_insights['dependency_relations'][token.dep_] = []\n",
    "            morphology_insights['dependency_relations'][token.dep_].append(token.text)\n",
    "        \n",
    "        return morphology_insights\n",
    "\n",
    "    def advanced_summary(self, text):\n",
    "        \"\"\"Generate comprehensive summary\"\"\"\n",
    "        preprocessed_text = self.preprocess_text(text)\n",
    "        morphological_data = self.morphological_analysis(preprocessed_text)\n",
    "        extractive_summary = self.generate_extractive_summary(preprocessed_text)\n",
    "        \n",
    "        return {\n",
    "            'preprocessed_text': preprocessed_text,\n",
    "            'morphological_insights': morphological_data,\n",
    "            'extractive_summary': extractive_summary\n",
    "        }\n",
    "\n",
    "    def visualize_semantic_graph(self, text):\n",
    "        \"\"\"Create semantic relationship graph\"\"\"\n",
    "        doc = self.nlp(text)\n",
    "        G = nx.DiGraph()\n",
    "        \n",
    "        for sent in doc.sents:\n",
    "            for token in sent:\n",
    "                if token.dep_ in ['nsubj', 'dobj', 'pobj']:\n",
    "                    G.add_node(token.text)\n",
    "                    \n",
    "                    # Connect related tokens\n",
    "                    for child in token.children:\n",
    "                        G.add_edge(token.text, child.text)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(G)\n",
    "        nx.draw_networkx_nodes(G, pos, node_color='lightblue')\n",
    "        nx.draw_networkx_edges(G, pos)\n",
    "        nx.draw_networkx_labels(G, pos)\n",
    "        plt.title(\"Semantic Relationship Graph\")\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('semantic_graph.png')\n",
    "        \n",
    "        return G\n",
    "\n",
    "def process_research_paper(pdf_path):\n",
    "    \"\"\"Process research paper and generate insights\"\"\"\n",
    "    processor = ResearchPaperProcessor()\n",
    "    \n",
    "    # Extract text\n",
    "    raw_text = processor.extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    # Generate comprehensive analysis\n",
    "    summary = processor.advanced_summary(raw_text)\n",
    "    \n",
    "    # Visualize semantic relationships\n",
    "    semantic_graph = processor.visualize_semantic_graph(raw_text)\n",
    "    \n",
    "    return summary, semantic_graph\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
